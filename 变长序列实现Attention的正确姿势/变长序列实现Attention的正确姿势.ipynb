{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导入需要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 模拟数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着模拟一组数据，假设batch_x是表示成单词id序列的输入，我们希望通过Attention机制，为一个实例中的每次词分配一个概率得分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "batch_x = torch.LongTensor(np.array([\n",
    "    [1, 2, 3, 0, 0, 0],\n",
    "    [1, 2, 3, 4, 5, 6],\n",
    "    [1, 2, 0, 0, 0, 0]]\n",
    "))\n",
    "mask = (batch_x > 0).float()\n",
    "batch_size = 3\n",
    "max_len = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置embedding层："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 6, 8])\n"
     ]
    }
   ],
   "source": [
    "alphabet_size = 7\n",
    "input_dim = 8\n",
    "embedding = nn.Embedding(alphabet_size, input_dim, padding_idx=0)\n",
    "batch_x_embed = embedding(batch_x)\n",
    "print(batch_x_embed.size())  # size=(3, 6, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 设置Attention层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_layer = nn.Linear(input_dim, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置该层只是为了模拟attention实现过程，实际任务中，attention的权重会根据loss进行更新。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 错误姿势"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先看一下错误的实现方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1241,  0.0665,  0.3636,  0.1486,  0.1486,  0.1486],\n",
      "        [ 0.1179,  0.0632,  0.3454,  0.2390,  0.1357,  0.0987],\n",
      "        [ 0.1581,  0.0848,  0.1893,  0.1893,  0.1893,  0.1893]])\n"
     ]
    }
   ],
   "source": [
    "batch_x_embed = batch_x_embed.view(-1, input_dim)\n",
    "att_score = attention_layer(batch_x_embed).view(batch_size, max_len)\n",
    "att_score *= mask\n",
    "att_weight = F.softmax(att_score, dim=1)\n",
    "print(att_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看到，padding处也有一个概率得分，而这些位置的值应该是0；如果直接将这部分置0，那么padding之前部分的概率之和就不再是1，所以这样的实现方式是错误的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 正确姿势"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来看一下正确的实现方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2239,  0.1201,  0.6560,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1179,  0.0632,  0.3454,  0.2390,  0.1357,  0.0987],\n",
      "        [ 0.6510,  0.3490,  0.0000,  0.0000,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "batch_x_embed = batch_x_embed.view(-1, input_dim)\n",
    "att_score = attention_layer(batch_x_embed).view(batch_size, max_len)\n",
    "att_score[mask==0] = -np.inf  # e**inf=0\n",
    "att_weight = F.softmax(att_score, dim=1)\n",
    "print(att_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出，padding部分的权重正确地计算为0，而padding之前的部分概率之和为1。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
