{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLSTM处理变长序列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设我们正在使用BLSTM模型处理句子分类的任务，BLSTM最后一个时刻的输出作为句子的表示。\n",
    "\n",
    "例如以下实例：\n",
    "\n",
    "    sentences = [['nice', 'day'], ['I', 'like', 'to', 'eat', 'apple'], ['can', 'a', 'can']]\n",
    "\n",
    "在实际操作时，考虑到计算性能，会将其padding成统一的长度（通常是一个batch中的最大长度）：\n",
    "\n",
    "    sentences = [\n",
    "        ['nice', 'day', '_PAD', '_PAD', '_PAD'],\n",
    "        ['I', 'like', 'to', 'eat', 'apple'],\n",
    "        ['can', 'a', 'can', '_PAD', '_PAD']\n",
    "    ]\n",
    "\n",
    "考虑序列`['nice', 'day', '_PAD', '_PAD', '_PAD']`，对于正向LSTM，我们仅需要在`day`处的`hidden state`；对于反向LSTM，仅需要从`day`编码到`nice`，`_PAD`处的值并不需要计算。\n",
    "\n",
    "PyTorch通过`torch.nn.utils.rnn.PackedSequence`类，以及以下两个函数处理上述变长序列问题：\n",
    "  - `torch.nn.utils.rnn.pack_padded_sequence`\n",
    "  - `torch.nn.utils.rnn.pad_packed_sequence`\n",
    "\n",
    "接下来，我们通过一组模拟数据，介绍PyTorch中双向LSTM处理变长序列的方法，并对其正确性作了检验。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 引入相关包"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先引入需要的包，并设置相关参数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "input_size = 64\n",
    "hidden_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化BLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着初始化BLSTM，为了方便演示，将`batch_first`设为`True`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(\n",
    "    input_size=input_size, hidden_size=hidden_size, num_layers=1,\n",
    "    bias=True, batch_first=True, bidirectional=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了验证PyTorch计算双向lstm输出的正确性，我们将正、方向LSTM的权重设为相同值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization is done!\n"
     ]
    }
   ],
   "source": [
    "weight_i = getattr(lstm, 'weight_ih_l{0}'.format(0))  # 正向: (W_ii|W_if|W_ig|W_io)\n",
    "weight_i_r = getattr(lstm, 'weight_ih_l{0}_reverse'.format(0))  # 反向: (W_ii|W_if|W_ig|W_io)\n",
    "weight_i_r.data.copy_(weight_i.data)\n",
    "\n",
    "weight_h = getattr(lstm, 'weight_hh_l{0}'.format(0))  # 正向: (W_hi|W_hf|W_ig|W_ho)\n",
    "weight_h_r = getattr(lstm, 'weight_hh_l{0}_reverse'.format(0))  # 反向: (W_hi|W_hf|W_hg|W_ho)\n",
    "weight_h_r.data.copy_(weight_h.data)\n",
    "\n",
    "# init bias\n",
    "bias_i = getattr(lstm, 'bias_hh_l{0}'.format(0))\n",
    "torch.nn.init.constant_(bias_i, 0.1)\n",
    "bias_h = getattr(lstm, 'bias_ih_l{0}'.format(0))\n",
    "torch.nn.init.constant_(bias_h, 0.1)\n",
    "\n",
    "bias_i_r = getattr(lstm, 'bias_hh_l{0}_reverse'.format(0))\n",
    "bias_i_r.data.copy_(bias_i)\n",
    "bias_h_r = getattr(lstm, 'bias_ih_l{0}_reverse'.format(0))\n",
    "bias_h_r.data.copy_(bias_h)\n",
    "\n",
    "print('Initialization is done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样，若某个序列的正、反向输入相同，则正、反方向最后一个时刻的输出应该一致；若反向LSTM计算了`_PAD`值，则输出结果会不一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建模拟数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 64])\n",
      "inputs_packed.data.size: torch.Size([10, 64])\n",
      "batch_sizes: tensor([ 3,  3,  2,  1,  1])\n"
     ]
    }
   ],
   "source": [
    "# 设置模拟输入数据\n",
    "sentences = [['nice', 'day'], ['I', 'like', 'to', 'eat', 'apple'], ['can', 'a', 'can']]\n",
    "test_sent_idx = 2  # 即['can', 'a', 'can']在sentences中的下标\n",
    "\n",
    "# 构建alphabet\n",
    "alphabet = {}\n",
    "index = 1\n",
    "for sentence in sentences:\n",
    "    for word in sentence:\n",
    "        if word not in alphabet:\n",
    "            alphabet[word] = index\n",
    "            index += 1\n",
    "voc_size = len(alphabet) + 1\n",
    "\n",
    "lengths = [len(s) for s in sentences]\n",
    "max_len = max(lengths)\n",
    "batch_size = len(sentences)\n",
    "\n",
    "inputs = np.zeros((batch_size, max_len), dtype='int32')\n",
    "for i, sentence in enumerate(sentences):\n",
    "    ids = list(map(lambda w: alphabet[w], sentence))\n",
    "    inputs[i, :lengths[i]] = ids\n",
    "inputs = torch.LongTensor(inputs)\n",
    "lengths = torch.LongTensor(lengths)\n",
    "\n",
    "# 按句子实际长度降序排序\n",
    "lengths, indices = torch.sort(lengths, descending=True)\n",
    "inputs = inputs[indices]\n",
    "\n",
    "# 设置embedding层，其中padding_idx表示padding值的编号\n",
    "embedding = nn.Embedding(voc_size, input_size, padding_idx=0)\n",
    "inputs = embedding(inputs)\n",
    "print(inputs.size())  # [3, 5, 64]\n",
    "\n",
    "inputs_packed = pack_padded_sequence(inputs, lengths, True)\n",
    "print('inputs_packed.data.size: {0}'.format(inputs_packed.data.size()))\n",
    "print('batch_sizes: {0}'.format(inputs_packed.batch_sizes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PackedSequence包含两个值，分别是`data`和`batch_sizes`。其中`data`根据`lengths`参数(即序列的实际长度)，记录了`inputs`中的tensor；`batch_sizes`长度等于实际长度的最大值，第`i`个值记录了第`i`时刻输入的batch size大小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算LSTM输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 100])\n"
     ]
    }
   ],
   "source": [
    "lstm_output, lstm_hidden = lstm(inputs_packed)\n",
    "lstm_hidden, lstm_cell_state = lstm_hidden[0], lstm_hidden[1]\n",
    "lstm_hidden = lstm_hidden.transpose(0, 1)\n",
    "print(lstm_hidden.size())  # torch.Size([batch_size, 2, hidden_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`lstm_hidden`和`lstm_cell_state`分别记录了正、反向最后一个时刻的`hidden state`和`cell state`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0625,  0.3070, -0.1729, -0.2379, -0.0848,  0.2300,  0.0295,\n",
      "          0.3735,  0.0307,  0.0912,  0.1333,  0.1341,  0.2309,  0.0204,\n",
      "         -0.0117,  0.1841,  0.1630,  0.4388,  0.0381,  0.0551,  0.1766,\n",
      "         -0.0415,  0.2978,  0.2218,  0.2320,  0.2776,  0.1525,  0.0690,\n",
      "          0.1018, -0.0519, -0.0773, -0.0358,  0.2640, -0.1285,  0.0648,\n",
      "          0.4546,  0.1061, -0.1082,  0.1457,  0.2305,  0.1844,  0.0870,\n",
      "          0.0993,  0.2757, -0.1717, -0.0297,  0.1028,  0.1016,  0.0387,\n",
      "          0.1672,  0.3925,  0.1485,  0.4487,  0.1449, -0.0291,  0.1742,\n",
      "          0.1658, -0.1843,  0.2707,  0.0617,  0.2482, -0.0416,  0.2566,\n",
      "          0.2021,  0.3221,  0.1658,  0.2014, -0.0924,  0.2102,  0.1036,\n",
      "          0.2312,  0.0517,  0.1294,  0.1551,  0.1262, -0.0096,  0.0372,\n",
      "          0.1485,  0.0742,  0.1351, -0.0850,  0.3438, -0.1237,  0.0226,\n",
      "         -0.1902,  0.0077,  0.1242,  0.0630, -0.0051,  0.0149, -0.0595,\n",
      "          0.1055,  0.2078,  0.3090,  0.2792,  0.2109, -0.0175,  0.0582,\n",
      "          0.0451,  0.0683],\n",
      "        [ 0.0625,  0.3070, -0.1729, -0.2379, -0.0848,  0.2300,  0.0295,\n",
      "          0.3735,  0.0307,  0.0912,  0.1333,  0.1341,  0.2309,  0.0204,\n",
      "         -0.0117,  0.1841,  0.1630,  0.4388,  0.0381,  0.0551,  0.1766,\n",
      "         -0.0415,  0.2978,  0.2218,  0.2320,  0.2776,  0.1525,  0.0690,\n",
      "          0.1018, -0.0519, -0.0773, -0.0358,  0.2640, -0.1285,  0.0648,\n",
      "          0.4546,  0.1061, -0.1082,  0.1457,  0.2305,  0.1844,  0.0870,\n",
      "          0.0993,  0.2757, -0.1717, -0.0297,  0.1028,  0.1016,  0.0387,\n",
      "          0.1672,  0.3925,  0.1485,  0.4487,  0.1449, -0.0291,  0.1742,\n",
      "          0.1658, -0.1843,  0.2707,  0.0617,  0.2482, -0.0416,  0.2566,\n",
      "          0.2021,  0.3221,  0.1658,  0.2014, -0.0924,  0.2102,  0.1036,\n",
      "          0.2312,  0.0517,  0.1294,  0.1551,  0.1262, -0.0096,  0.0372,\n",
      "          0.1485,  0.0742,  0.1351, -0.0850,  0.3438, -0.1237,  0.0226,\n",
      "         -0.1902,  0.0077,  0.1242,  0.0630, -0.0051,  0.0149, -0.0595,\n",
      "          0.1055,  0.2078,  0.3090,  0.2792,  0.2109, -0.0175,  0.0582,\n",
      "          0.0451,  0.0683]])\n"
     ]
    }
   ],
   "source": [
    "# lstm_hidden还原为原来的顺序\n",
    "_, indices_recover = torch.sort(indices)\n",
    "lstm_hidden_recover = lstm_hidden[indices_recover]\n",
    "\n",
    "# 句子['can', 'a', 'can']正反向lstm最后一个时刻的输出\n",
    "print(lstm_hidden_recover[test_sent_idx])  # [2, 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出正、反向的输出一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`lstm_output`记录的是BLSTM每个时刻的输出，根据句子实际长度，也可以取出在最后一个时刻的输出："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 200])\n",
      "tensor([ 0.0625,  0.3070, -0.1729, -0.2379, -0.0848,  0.2300,  0.0295,\n",
      "         0.3735,  0.0307,  0.0912,  0.1333,  0.1341,  0.2309,  0.0204,\n",
      "        -0.0117,  0.1841,  0.1630,  0.4388,  0.0381,  0.0551,  0.1766,\n",
      "        -0.0415,  0.2978,  0.2218,  0.2320,  0.2776,  0.1525,  0.0690,\n",
      "         0.1018, -0.0519, -0.0773, -0.0358,  0.2640, -0.1285,  0.0648,\n",
      "         0.4546,  0.1061, -0.1082,  0.1457,  0.2305,  0.1844,  0.0870,\n",
      "         0.0993,  0.2757, -0.1717, -0.0297,  0.1028,  0.1016,  0.0387,\n",
      "         0.1672,  0.3925,  0.1485,  0.4487,  0.1449, -0.0291,  0.1742,\n",
      "         0.1658, -0.1843,  0.2707,  0.0617,  0.2482, -0.0416,  0.2566,\n",
      "         0.2021,  0.3221,  0.1658,  0.2014, -0.0924,  0.2102,  0.1036,\n",
      "         0.2312,  0.0517,  0.1294,  0.1551,  0.1262, -0.0096,  0.0372,\n",
      "         0.1485,  0.0742,  0.1351, -0.0850,  0.3438, -0.1237,  0.0226,\n",
      "        -0.1902,  0.0077,  0.1242,  0.0630, -0.0051,  0.0149, -0.0595,\n",
      "         0.1055,  0.2078,  0.3090,  0.2792,  0.2109, -0.0175,  0.0582,\n",
      "         0.0451,  0.0683])\n",
      "tensor([ 0.0625,  0.3070, -0.1729, -0.2379, -0.0848,  0.2300,  0.0295,\n",
      "         0.3735,  0.0307,  0.0912,  0.1333,  0.1341,  0.2309,  0.0204,\n",
      "        -0.0117,  0.1841,  0.1630,  0.4388,  0.0381,  0.0551,  0.1766,\n",
      "        -0.0415,  0.2978,  0.2218,  0.2320,  0.2776,  0.1525,  0.0690,\n",
      "         0.1018, -0.0519, -0.0773, -0.0358,  0.2640, -0.1285,  0.0648,\n",
      "         0.4546,  0.1061, -0.1082,  0.1457,  0.2305,  0.1844,  0.0870,\n",
      "         0.0993,  0.2757, -0.1717, -0.0297,  0.1028,  0.1016,  0.0387,\n",
      "         0.1672,  0.3925,  0.1485,  0.4487,  0.1449, -0.0291,  0.1742,\n",
      "         0.1658, -0.1843,  0.2707,  0.0617,  0.2482, -0.0416,  0.2566,\n",
      "         0.2021,  0.3221,  0.1658,  0.2014, -0.0924,  0.2102,  0.1036,\n",
      "         0.2312,  0.0517,  0.1294,  0.1551,  0.1262, -0.0096,  0.0372,\n",
      "         0.1485,  0.0742,  0.1351, -0.0850,  0.3438, -0.1237,  0.0226,\n",
      "        -0.1902,  0.0077,  0.1242,  0.0630, -0.0051,  0.0149, -0.0595,\n",
      "         0.1055,  0.2078,  0.3090,  0.2792,  0.2109, -0.0175,  0.0582,\n",
      "         0.0451,  0.0683])\n"
     ]
    }
   ],
   "source": [
    "# 还原为原形状\n",
    "lstm_output_pad, lengths = pad_packed_sequence(lstm_output, batch_first=True)\n",
    "lstm_output_pad_recover = lstm_output_pad[indices_recover]\n",
    "lengths_recover = lengths[indices_recover]\n",
    "print(lstm_output_pad.size())  # size=[3, 5, 200]\n",
    "\n",
    "# 句子['can', 'a', 'can']正反向lstm最后一个时刻的输出\n",
    "hidden_last = lstm_output_pad_recover[test_sent_idx][lengths_recover[test_sent_idx]-1][:hidden_size]  # 正向\n",
    "hidden_last_r = lstm_output_pad_recover[test_sent_idx][0][hidden_size:]  # 反向\n",
    "print(hidden_last)\n",
    "print(hidden_last_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "取出的值与`lstm_hidden`的值相等。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
