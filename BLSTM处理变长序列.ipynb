{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLSTM处理变长序列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设我们正在使用BLSTM模型处理句子分类的任务，BLSTM最后一个时刻的输出作为句子的表示。\n",
    "\n",
    "例如以下实例：\n",
    "\n",
    "    sentences = [['nice', 'day'], ['I', 'like', 'to', 'eat', 'apple'], ['can', 'a', 'can']]\n",
    "\n",
    "在实际操作时，考虑到计算性能，会将其padding成统一的长度（通常是一个batch中的最大长度）：\n",
    "\n",
    "    sentences = [\n",
    "        ['nice', 'day', '_PAD', '_PAD', '_PAD'],\n",
    "        ['I', 'like', 'to', 'eat', 'apple'],\n",
    "        ['can', 'a', 'can', '_PAD', '_PAD']\n",
    "    ]\n",
    "\n",
    "考虑序列`['nice', 'day', '_PAD', '_PAD', '_PAD']`，对于正向LSTM，我们仅需要在`day`处的`hidden state`；对于反向LSTM，仅需要从`day`编码到`nice`，`_PAD`处的值并不需要计算。\n",
    "\n",
    "PyTorch通过`torch.nn.utils.rnn.PackedSequence`类，以及以下两个函数处理上述变长序列问题：\n",
    "  - `torch.nn.utils.rnn.pack_padded_sequence`\n",
    "  - `torch.nn.utils.rnn.pad_packed_sequence`\n",
    "\n",
    "接下来，我们通过一组模拟数据，介绍PyTorch中双向LSTM处理变长序列的方法，并对其正确性作了检验。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 引入相关包"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先引入需要的包，并设置相关参数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "input_size = 64\n",
    "hidden_size = 100\n",
    "voc_size = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化BLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着初始化BLSTM，这里不使用bias是为了方便权重的初始化，同时为了方便演示，将`batch_first`设为`True`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(\n",
    "    input_size=input_size, hidden_size=hidden_size, num_layers=1,\n",
    "    bias=False, batch_first=True, bidirectional=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了验证PyTorch计算双向lstm输出的正确性，我们将正、方向LSTM的权重设为相同值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization is done!\n"
     ]
    }
   ],
   "source": [
    "weight_i = getattr(lstm, 'weight_ih_l{0}'.format(0))  # 正向: (W_ii|W_if|W_ig|W_io)\n",
    "weight_i_r = getattr(lstm, 'weight_ih_l{0}_reverse'.format(0))  # 反向: (W_ii|W_if|W_ig|W_io)\n",
    "weight_i_r.data.copy_(weight_i.data)\n",
    "\n",
    "weight_h = getattr(lstm, 'weight_hh_l{0}'.format(0))  # 正向: (W_hi|W_hf|W_ig|W_ho)\n",
    "weight_h_r = getattr(lstm, 'weight_hh_l{0}_reverse'.format(0))  # 反向: (W_hi|W_hf|W_hg|W_ho)\n",
    "weight_h_r.data.copy_(weight_h.data)\n",
    "\n",
    "print('Initialization is done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样，若某个序列的正、反向输入相同，则正、反方向最后一个时刻的输出应该一致；若反向LSTM计算了`_PAD`值，则输出结果会不一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建模拟数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 64])\n",
      "inputs_packed.data.size: torch.Size([10, 64])\n",
      "batch_sizes: tensor([ 3,  3,  2,  1,  1])\n"
     ]
    }
   ],
   "source": [
    "# 设置模拟输入数据\n",
    "sentences = [['nice', 'day'], ['I', 'like', 'to', 'eat', 'apple'], ['can', 'a', 'can']]\n",
    "test_sent_idx = 2  # 即['can', 'a', 'can']在sentences中的下标\n",
    "\n",
    "# 构建alphabet\n",
    "alphabet = {}\n",
    "index = 1\n",
    "for sentence in sentences:\n",
    "    for word in sentence:\n",
    "        if word not in alphabet:\n",
    "            alphabet[word] = index\n",
    "            index += 1\n",
    "lengths = [len(s) for s in sentences]\n",
    "max_len = max(lengths)\n",
    "batch_size = len(sentences)\n",
    "\n",
    "inputs = np.zeros((batch_size, max_len), dtype='int32')\n",
    "for i, sentence in enumerate(sentences):\n",
    "    ids = list(map(lambda w: alphabet[w], sentence))\n",
    "    inputs[i, :lengths[i]] = ids\n",
    "inputs = torch.LongTensor(inputs)\n",
    "lengths = torch.LongTensor(lengths)\n",
    "\n",
    "# 按句子实际长度降序排序\n",
    "lengths, indices = torch.sort(lengths, descending=True)\n",
    "inputs = inputs[indices]\n",
    "_, indices_recover = torch.sort(indices)  # 用于还原为原序列\n",
    "\n",
    "# 设置embedding层，其中padding_idx表示padding值的编号\n",
    "embedding = nn.Embedding(voc_size, input_size, padding_idx=0)\n",
    "inputs = embedding(inputs)\n",
    "print(inputs.size())  # [3, 5, 64]\n",
    "\n",
    "inputs_packed = pack_padded_sequence(inputs, lengths, True)\n",
    "print('inputs_packed.data.size: {0}'.format(inputs_packed.data.size()))\n",
    "print('batch_sizes: {0}'.format(inputs_packed.batch_sizes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PackedSequence包含两个值，分别是`data`和`batch_sizes`。其中`data`根据`lengths`参数(即序列的实际长度)，记录了`inputs`中的tensor；`batch_sizes`长度等于实际长度的最大值，第`i`个值记录了第`i`时刻输入的batch size大小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算LSTM输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 100])\n"
     ]
    }
   ],
   "source": [
    "lstm_output, lstm_hidden = lstm(inputs_packed)\n",
    "lstm_hidden, lstm_cell_state = lstm_hidden[0], lstm_hidden[1]\n",
    "lstm_hidden = lstm_hidden.transpose(0, 1)\n",
    "print(lstm_hidden.size())  # torch.Size([batch_size, 2, hidden_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`lstm_hidden`和`lstm_cell_state`分别记录了正、反向最后一个时刻的`hidden state`和`cell state`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3368, -0.0240,  0.1080, -0.1191,  0.0403, -0.2525, -0.1497,\n",
      "         -0.1395, -0.0139, -0.0174,  0.0452, -0.0374,  0.0151, -0.0664,\n",
      "          0.1031, -0.2861,  0.0777,  0.0341,  0.2238, -0.0606,  0.1187,\n",
      "         -0.0912, -0.1714, -0.2521, -0.0669, -0.0877, -0.0448,  0.1882,\n",
      "          0.2066, -0.0237,  0.2246, -0.0791,  0.1213, -0.0267,  0.0449,\n",
      "         -0.1667, -0.2509, -0.0138,  0.1489,  0.0431, -0.1908,  0.0314,\n",
      "          0.0225,  0.0544,  0.0571,  0.0266, -0.1087,  0.0524, -0.0433,\n",
      "         -0.0243,  0.0668,  0.0219, -0.1165, -0.1460, -0.1115, -0.0206,\n",
      "          0.1466,  0.0416, -0.2200,  0.0825, -0.0216, -0.1909, -0.0777,\n",
      "          0.2464,  0.1675, -0.0917,  0.0665, -0.0184,  0.0786,  0.0604,\n",
      "          0.0275, -0.0962, -0.0110,  0.0853,  0.1375,  0.0879,  0.0941,\n",
      "          0.0529, -0.0430,  0.1885, -0.0014, -0.1174,  0.0093,  0.2019,\n",
      "         -0.1814,  0.0357,  0.0561,  0.1409,  0.0529,  0.1508, -0.0638,\n",
      "         -0.3775, -0.0084,  0.0304, -0.1446, -0.0993, -0.1134, -0.2885,\n",
      "         -0.0305,  0.0846],\n",
      "        [-0.3368, -0.0240,  0.1080, -0.1191,  0.0403, -0.2525, -0.1497,\n",
      "         -0.1395, -0.0139, -0.0174,  0.0452, -0.0374,  0.0151, -0.0664,\n",
      "          0.1031, -0.2861,  0.0777,  0.0341,  0.2238, -0.0606,  0.1187,\n",
      "         -0.0912, -0.1714, -0.2521, -0.0669, -0.0877, -0.0448,  0.1882,\n",
      "          0.2066, -0.0237,  0.2246, -0.0791,  0.1213, -0.0267,  0.0449,\n",
      "         -0.1667, -0.2509, -0.0138,  0.1489,  0.0431, -0.1908,  0.0314,\n",
      "          0.0225,  0.0544,  0.0571,  0.0266, -0.1087,  0.0524, -0.0433,\n",
      "         -0.0243,  0.0668,  0.0219, -0.1165, -0.1460, -0.1115, -0.0206,\n",
      "          0.1466,  0.0416, -0.2200,  0.0825, -0.0216, -0.1909, -0.0777,\n",
      "          0.2464,  0.1675, -0.0917,  0.0665, -0.0184,  0.0786,  0.0604,\n",
      "          0.0275, -0.0962, -0.0110,  0.0853,  0.1375,  0.0879,  0.0941,\n",
      "          0.0529, -0.0430,  0.1885, -0.0014, -0.1174,  0.0093,  0.2019,\n",
      "         -0.1814,  0.0357,  0.0561,  0.1409,  0.0529,  0.1508, -0.0638,\n",
      "         -0.3775, -0.0084,  0.0304, -0.1446, -0.0993, -0.1134, -0.2885,\n",
      "         -0.0305,  0.0846]])\n"
     ]
    }
   ],
   "source": [
    "# lstm_hidden还原为原来的顺序\n",
    "lstm_hidden_recover = lstm_hidden[indices_recover]\n",
    "print(lstm_hidden_recover[test_sent_idx])  # [2, 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出正、反向的输出一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`lstm_output`记录的是BLSTM每个时刻的输出，根据句子实际长度，也可以取出在最后一个时刻的输出："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 200])\n",
      "tensor([-0.3368, -0.0240,  0.1080, -0.1191,  0.0403, -0.2525, -0.1497,\n",
      "        -0.1395, -0.0139, -0.0174,  0.0452, -0.0374,  0.0151, -0.0664,\n",
      "         0.1031, -0.2861,  0.0777,  0.0341,  0.2238, -0.0606,  0.1187,\n",
      "        -0.0912, -0.1714, -0.2521, -0.0669, -0.0877, -0.0448,  0.1882,\n",
      "         0.2066, -0.0237,  0.2246, -0.0791,  0.1213, -0.0267,  0.0449,\n",
      "        -0.1667, -0.2509, -0.0138,  0.1489,  0.0431, -0.1908,  0.0314,\n",
      "         0.0225,  0.0544,  0.0571,  0.0266, -0.1087,  0.0524, -0.0433,\n",
      "        -0.0243,  0.0668,  0.0219, -0.1165, -0.1460, -0.1115, -0.0206,\n",
      "         0.1466,  0.0416, -0.2200,  0.0825, -0.0216, -0.1909, -0.0777,\n",
      "         0.2464,  0.1675, -0.0917,  0.0665, -0.0184,  0.0786,  0.0604,\n",
      "         0.0275, -0.0962, -0.0110,  0.0853,  0.1375,  0.0879,  0.0941,\n",
      "         0.0529, -0.0430,  0.1885, -0.0014, -0.1174,  0.0093,  0.2019,\n",
      "        -0.1814,  0.0357,  0.0561,  0.1409,  0.0529,  0.1508, -0.0638,\n",
      "        -0.3775, -0.0084,  0.0304, -0.1446, -0.0993, -0.1134, -0.2885,\n",
      "        -0.0305,  0.0846])\n",
      "tensor([-0.3368, -0.0240,  0.1080, -0.1191,  0.0403, -0.2525, -0.1497,\n",
      "        -0.1395, -0.0139, -0.0174,  0.0452, -0.0374,  0.0151, -0.0664,\n",
      "         0.1031, -0.2861,  0.0777,  0.0341,  0.2238, -0.0606,  0.1187,\n",
      "        -0.0912, -0.1714, -0.2521, -0.0669, -0.0877, -0.0448,  0.1882,\n",
      "         0.2066, -0.0237,  0.2246, -0.0791,  0.1213, -0.0267,  0.0449,\n",
      "        -0.1667, -0.2509, -0.0138,  0.1489,  0.0431, -0.1908,  0.0314,\n",
      "         0.0225,  0.0544,  0.0571,  0.0266, -0.1087,  0.0524, -0.0433,\n",
      "        -0.0243,  0.0668,  0.0219, -0.1165, -0.1460, -0.1115, -0.0206,\n",
      "         0.1466,  0.0416, -0.2200,  0.0825, -0.0216, -0.1909, -0.0777,\n",
      "         0.2464,  0.1675, -0.0917,  0.0665, -0.0184,  0.0786,  0.0604,\n",
      "         0.0275, -0.0962, -0.0110,  0.0853,  0.1375,  0.0879,  0.0941,\n",
      "         0.0529, -0.0430,  0.1885, -0.0014, -0.1174,  0.0093,  0.2019,\n",
      "        -0.1814,  0.0357,  0.0561,  0.1409,  0.0529,  0.1508, -0.0638,\n",
      "        -0.3775, -0.0084,  0.0304, -0.1446, -0.0993, -0.1134, -0.2885,\n",
      "        -0.0305,  0.0846])\n"
     ]
    }
   ],
   "source": [
    "# 还原为原形状\n",
    "lstm_output_pad, lengths = pad_packed_sequence(lstm_output, batch_first=True)\n",
    "print(lstm_output_pad.size())  # size=[4, 9, 200]\n",
    "\n",
    "# sent_idx=1句子正反向lstm输出\n",
    "hidden_last = lstm_output_pad[sent_idx][lengths[sent_idx]-1][:hidden_size]  # 正向\n",
    "hidden_last_r = lstm_output_pad[sent_idx][0][hidden_size:]  # 反向\n",
    "print(hidden_last)\n",
    "print(hidden_last_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "取出的值与`lstm_hidden`的值相等。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
