{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLSTM处理变长序列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设我们正在使用BLSTM模型处理句子分类的任务，BLSTM最后一个时刻的输出作为句子的表示。\n",
    "\n",
    "例如以下实例：\n",
    "\n",
    "    sentences = [['nice', 'day'], ['I', 'like', 'to', 'eat', 'apple'], ['can', 'a', 'can']]\n",
    "\n",
    "在实际操作时，考虑到计算性能，会将其padding成统一的长度（通常是一个batch中的最大长度）：\n",
    "\n",
    "    sentences = [\n",
    "        ['nice', 'day', '_PAD', '_PAD', '_PAD'],\n",
    "        ['I', 'like', 'to', 'eat', 'apple'],\n",
    "        ['can', 'a', 'can', '_PAD', '_PAD']\n",
    "    ]\n",
    "\n",
    "考虑序列`['nice', 'day', '_PAD', '_PAD', '_PAD']`，对于正向LSTM，我们仅需要在`day`处的`hidden state`；对于反向LSTM，仅需要从`day`编码到`nice`，`_PAD`处的值并不需要计算。\n",
    "\n",
    "PyTorch通过`torch.nn.utils.rnn.PackedSequence`类，以及以下两个函数处理上述变长序列问题：\n",
    "  - `torch.nn.utils.rnn.pack_padded_sequence`\n",
    "  - `torch.nn.utils.rnn.pad_packed_sequence`\n",
    "\n",
    "接下来，我们通过一组模拟数据，介绍PyTorch中双向LSTM处理变长序列的方法，并对其正确性作了检验。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 引入相关包"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先引入需要的包，并设置相关参数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "input_size = 64\n",
    "hidden_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化BLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着初始化BLSTM，这里不使用bias是为了简化权重的初始化，同时为了方便演示，将`batch_first`设为`True`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(\n",
    "    input_size=input_size, hidden_size=hidden_size, num_layers=1,\n",
    "    bias=False, batch_first=True, bidirectional=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了验证PyTorch计算双向lstm输出的正确性，我们将正、方向LSTM的权重设为相同值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization is done!\n"
     ]
    }
   ],
   "source": [
    "weight_i = getattr(lstm, 'weight_ih_l{0}'.format(0))  # 正向: (W_ii|W_if|W_ig|W_io)\n",
    "weight_i_r = getattr(lstm, 'weight_ih_l{0}_reverse'.format(0))  # 反向: (W_ii|W_if|W_ig|W_io)\n",
    "weight_i_r.data.copy_(weight_i.data)\n",
    "\n",
    "weight_h = getattr(lstm, 'weight_hh_l{0}'.format(0))  # 正向: (W_hi|W_hf|W_ig|W_ho)\n",
    "weight_h_r = getattr(lstm, 'weight_hh_l{0}_reverse'.format(0))  # 反向: (W_hi|W_hf|W_hg|W_ho)\n",
    "weight_h_r.data.copy_(weight_h.data)\n",
    "\n",
    "print('Initialization is done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样，若某个序列的正、反向输入相同，则正、反方向最后一个时刻的输出应该一致；若反向LSTM计算了`_PAD`值，则输出结果会不一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建模拟数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 64])\n",
      "inputs_packed.data.size: torch.Size([10, 64])\n",
      "batch_sizes: tensor([ 3,  3,  2,  1,  1])\n"
     ]
    }
   ],
   "source": [
    "# 设置模拟输入数据\n",
    "sentences = [['nice', 'day'], ['I', 'like', 'to', 'eat', 'apple'], ['can', 'a', 'can']]\n",
    "test_sent_idx = 2  # 即['can', 'a', 'can']在sentences中的下标\n",
    "\n",
    "# 构建alphabet\n",
    "alphabet = {}\n",
    "index = 1\n",
    "for sentence in sentences:\n",
    "    for word in sentence:\n",
    "        if word not in alphabet:\n",
    "            alphabet[word] = index\n",
    "            index += 1\n",
    "voc_size = len(alphabet) + 1\n",
    "\n",
    "lengths = [len(s) for s in sentences]\n",
    "max_len = max(lengths)\n",
    "batch_size = len(sentences)\n",
    "\n",
    "inputs = np.zeros((batch_size, max_len), dtype='int32')\n",
    "for i, sentence in enumerate(sentences):\n",
    "    ids = list(map(lambda w: alphabet[w], sentence))\n",
    "    inputs[i, :lengths[i]] = ids\n",
    "inputs = torch.LongTensor(inputs)\n",
    "lengths = torch.LongTensor(lengths)\n",
    "\n",
    "# 按句子实际长度降序排序\n",
    "lengths, indices = torch.sort(lengths, descending=True)\n",
    "inputs = inputs[indices]\n",
    "\n",
    "# 设置embedding层，其中padding_idx表示padding值的编号\n",
    "embedding = nn.Embedding(voc_size, input_size, padding_idx=0)\n",
    "inputs = embedding(inputs)\n",
    "print(inputs.size())  # [3, 5, 64]\n",
    "\n",
    "inputs_packed = pack_padded_sequence(inputs, lengths, True)\n",
    "print('inputs_packed.data.size: {0}'.format(inputs_packed.data.size()))\n",
    "print('batch_sizes: {0}'.format(inputs_packed.batch_sizes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PackedSequence包含两个值，分别是`data`和`batch_sizes`。其中`data`根据`lengths`参数(即序列的实际长度)，记录了`inputs`中的tensor；`batch_sizes`长度等于实际长度的最大值，第`i`个值记录了第`i`时刻输入的batch size大小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算LSTM输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 100])\n"
     ]
    }
   ],
   "source": [
    "lstm_output, lstm_hidden = lstm(inputs_packed)\n",
    "lstm_hidden, lstm_cell_state = lstm_hidden[0], lstm_hidden[1]\n",
    "lstm_hidden = lstm_hidden.transpose(0, 1)\n",
    "print(lstm_hidden.size())  # torch.Size([batch_size, 2, hidden_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`lstm_hidden`和`lstm_cell_state`分别记录了正、反向最后一个时刻的`hidden state`和`cell state`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0228,  0.1170, -0.0451, -0.0568,  0.1403,  0.0784, -0.0243,\n",
      "         -0.1485,  0.0261, -0.1661,  0.0351,  0.0388,  0.0242,  0.1387,\n",
      "          0.0971,  0.1638,  0.1037, -0.0110, -0.1212, -0.0216,  0.0468,\n",
      "          0.0197,  0.0597,  0.0338,  0.0693, -0.0474,  0.2046, -0.0400,\n",
      "          0.1145, -0.0909,  0.0937,  0.1635,  0.2097, -0.0552,  0.0778,\n",
      "         -0.1668, -0.1519, -0.0804,  0.1745, -0.1755, -0.2348,  0.1344,\n",
      "         -0.1358,  0.2272,  0.1237,  0.0951,  0.1680,  0.2202, -0.0688,\n",
      "         -0.0385, -0.0741, -0.0883,  0.0944,  0.2821, -0.0596,  0.0859,\n",
      "          0.0558,  0.1748, -0.1993, -0.0317, -0.1196, -0.0811, -0.0041,\n",
      "         -0.0472, -0.0441, -0.0055, -0.1401, -0.0508,  0.1898, -0.0252,\n",
      "          0.2173,  0.0231,  0.1498, -0.0523,  0.2766,  0.2083, -0.1071,\n",
      "          0.1980,  0.0665, -0.1066,  0.0773,  0.0345,  0.1416, -0.2942,\n",
      "          0.1971, -0.0078,  0.0528, -0.2320, -0.1157, -0.1126,  0.2077,\n",
      "         -0.2817, -0.0535,  0.0617, -0.0845,  0.0861, -0.0674,  0.1333,\n",
      "         -0.1083,  0.0050],\n",
      "        [-0.0228,  0.1170, -0.0451, -0.0568,  0.1403,  0.0784, -0.0243,\n",
      "         -0.1485,  0.0261, -0.1661,  0.0351,  0.0388,  0.0242,  0.1387,\n",
      "          0.0971,  0.1638,  0.1037, -0.0110, -0.1212, -0.0216,  0.0468,\n",
      "          0.0197,  0.0597,  0.0338,  0.0693, -0.0474,  0.2046, -0.0400,\n",
      "          0.1145, -0.0909,  0.0937,  0.1635,  0.2097, -0.0552,  0.0778,\n",
      "         -0.1668, -0.1519, -0.0804,  0.1745, -0.1755, -0.2348,  0.1344,\n",
      "         -0.1358,  0.2272,  0.1237,  0.0951,  0.1680,  0.2202, -0.0688,\n",
      "         -0.0385, -0.0741, -0.0883,  0.0944,  0.2821, -0.0596,  0.0859,\n",
      "          0.0558,  0.1748, -0.1993, -0.0317, -0.1196, -0.0811, -0.0041,\n",
      "         -0.0472, -0.0441, -0.0055, -0.1401, -0.0508,  0.1898, -0.0252,\n",
      "          0.2173,  0.0231,  0.1498, -0.0523,  0.2766,  0.2083, -0.1071,\n",
      "          0.1980,  0.0665, -0.1066,  0.0773,  0.0345,  0.1416, -0.2942,\n",
      "          0.1971, -0.0078,  0.0528, -0.2320, -0.1157, -0.1126,  0.2077,\n",
      "         -0.2817, -0.0535,  0.0617, -0.0845,  0.0861, -0.0674,  0.1333,\n",
      "         -0.1083,  0.0050]])\n"
     ]
    }
   ],
   "source": [
    "# lstm_hidden还原为原来的顺序\n",
    "_, indices_recover = torch.sort(indices)\n",
    "lstm_hidden_recover = lstm_hidden[indices_recover]\n",
    "\n",
    "# 句子['can', 'a', 'can']正反向lstm最后一个时刻的输出\n",
    "print(lstm_hidden_recover[test_sent_idx])  # [2, 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出正、反向的输出一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`lstm_output`记录的是BLSTM每个时刻的输出，根据句子实际长度，也可以取出在最后一个时刻的输出："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 200])\n",
      "tensor([-0.0228,  0.1170, -0.0451, -0.0568,  0.1403,  0.0784, -0.0243,\n",
      "        -0.1485,  0.0261, -0.1661,  0.0351,  0.0388,  0.0242,  0.1387,\n",
      "         0.0971,  0.1638,  0.1037, -0.0110, -0.1212, -0.0216,  0.0468,\n",
      "         0.0197,  0.0597,  0.0338,  0.0693, -0.0474,  0.2046, -0.0400,\n",
      "         0.1145, -0.0909,  0.0937,  0.1635,  0.2097, -0.0552,  0.0778,\n",
      "        -0.1668, -0.1519, -0.0804,  0.1745, -0.1755, -0.2348,  0.1344,\n",
      "        -0.1358,  0.2272,  0.1237,  0.0951,  0.1680,  0.2202, -0.0688,\n",
      "        -0.0385, -0.0741, -0.0883,  0.0944,  0.2821, -0.0596,  0.0859,\n",
      "         0.0558,  0.1748, -0.1993, -0.0317, -0.1196, -0.0811, -0.0041,\n",
      "        -0.0472, -0.0441, -0.0055, -0.1401, -0.0508,  0.1898, -0.0252,\n",
      "         0.2173,  0.0231,  0.1498, -0.0523,  0.2766,  0.2083, -0.1071,\n",
      "         0.1980,  0.0665, -0.1066,  0.0773,  0.0345,  0.1416, -0.2942,\n",
      "         0.1971, -0.0078,  0.0528, -0.2320, -0.1157, -0.1126,  0.2077,\n",
      "        -0.2817, -0.0535,  0.0617, -0.0845,  0.0861, -0.0674,  0.1333,\n",
      "        -0.1083,  0.0050])\n",
      "tensor([-0.0228,  0.1170, -0.0451, -0.0568,  0.1403,  0.0784, -0.0243,\n",
      "        -0.1485,  0.0261, -0.1661,  0.0351,  0.0388,  0.0242,  0.1387,\n",
      "         0.0971,  0.1638,  0.1037, -0.0110, -0.1212, -0.0216,  0.0468,\n",
      "         0.0197,  0.0597,  0.0338,  0.0693, -0.0474,  0.2046, -0.0400,\n",
      "         0.1145, -0.0909,  0.0937,  0.1635,  0.2097, -0.0552,  0.0778,\n",
      "        -0.1668, -0.1519, -0.0804,  0.1745, -0.1755, -0.2348,  0.1344,\n",
      "        -0.1358,  0.2272,  0.1237,  0.0951,  0.1680,  0.2202, -0.0688,\n",
      "        -0.0385, -0.0741, -0.0883,  0.0944,  0.2821, -0.0596,  0.0859,\n",
      "         0.0558,  0.1748, -0.1993, -0.0317, -0.1196, -0.0811, -0.0041,\n",
      "        -0.0472, -0.0441, -0.0055, -0.1401, -0.0508,  0.1898, -0.0252,\n",
      "         0.2173,  0.0231,  0.1498, -0.0523,  0.2766,  0.2083, -0.1071,\n",
      "         0.1980,  0.0665, -0.1066,  0.0773,  0.0345,  0.1416, -0.2942,\n",
      "         0.1971, -0.0078,  0.0528, -0.2320, -0.1157, -0.1126,  0.2077,\n",
      "        -0.2817, -0.0535,  0.0617, -0.0845,  0.0861, -0.0674,  0.1333,\n",
      "        -0.1083,  0.0050])\n"
     ]
    }
   ],
   "source": [
    "# 还原为原形状\n",
    "lstm_output_pad, lengths = pad_packed_sequence(lstm_output, batch_first=True)\n",
    "lstm_output_pad_recover = lstm_output_pad[indices_recover]\n",
    "lengths_recover = lengths[indices_recover]\n",
    "print(lstm_output_pad.size())  # size=[4, 9, 200]\n",
    "\n",
    "# 句子['can', 'a', 'can']正反向lstm最后一个时刻的输出\n",
    "hidden_last = lstm_output_pad_recover[test_sent_idx][lengths_recover[test_sent_idx]-1][:hidden_size]  # 正向\n",
    "hidden_last_r = lstm_output_pad_recover[test_sent_idx][0][hidden_size:]  # 反向\n",
    "print(hidden_last)\n",
    "print(hidden_last_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "取出的值与`lstm_hidden`的值相等。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
