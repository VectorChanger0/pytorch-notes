{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLSTM处理变长序列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设我们正在使用BLSTM模型处理句子分类的任务，BLSTM最后一个时刻的输出作为句子的表示。\n",
    "\n",
    "例如以下实例：\n",
    "\n",
    "    sentences = [['nice', 'day'], ['I', 'like', 'to', 'eat', 'apple'], ['can', 'a', 'can']]\n",
    "\n",
    "在实际操作时，考虑到计算性能，会将其padding成统一的长度（通常是一个batch中的最大长度）：\n",
    "\n",
    "    sentences = [\n",
    "        ['nice', 'day', '_PAD', '_PAD', '_PAD'],\n",
    "        ['I', 'like', 'to', 'eat', 'apple'],\n",
    "        ['can', 'a', 'can', '_PAD', '_PAD']\n",
    "    ]\n",
    "\n",
    "考虑序列`['nice', 'day', '_PAD', '_PAD', '_PAD']`，对于正向LSTM，我们仅需要在`day`处的`hidden state`；对于反向LSTM，仅需要从`day`编码到`nice`，`_PAD`处的值并不需要计算。\n",
    "\n",
    "PyTorch通过`torch.nn.utils.rnn.PackedSequence`类，以及以下两个函数处理上述变长序列问题：\n",
    "  - `torch.nn.utils.rnn.pack_padded_sequence`\n",
    "  - `torch.nn.utils.rnn.pad_packed_sequence`\n",
    "\n",
    "接下来，我们通过一组模拟数据，介绍PyTorch中双向LSTM处理变长序列的方法，并对其正确性作了检验。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 引入相关包"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先引入需要的包，并设置相关参数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "input_size = 64\n",
    "hidden_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化BLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着初始化BLSTM，为了方便演示，将`batch_first`设为`True`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(\n",
    "    input_size=input_size, hidden_size=hidden_size, num_layers=1,\n",
    "    bias=True, batch_first=True, bidirectional=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了验证PyTorch计算双向lstm输出的正确性，我们将正、方向LSTM的权重及偏置设为相同值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization is done!\n"
     ]
    }
   ],
   "source": [
    "weight_i = getattr(lstm, 'weight_ih_l{0}'.format(0))  # 正向: (W_ii|W_if|W_ig|W_io)\n",
    "weight_i_r = getattr(lstm, 'weight_ih_l{0}_reverse'.format(0))  # 反向: (W_ii|W_if|W_ig|W_io)\n",
    "weight_i_r.data.copy_(weight_i.data)\n",
    "\n",
    "weight_h = getattr(lstm, 'weight_hh_l{0}'.format(0))  # 正向: (W_hi|W_hf|W_ig|W_ho)\n",
    "weight_h_r = getattr(lstm, 'weight_hh_l{0}_reverse'.format(0))  # 反向: (W_hi|W_hf|W_hg|W_ho)\n",
    "weight_h_r.data.copy_(weight_h.data)\n",
    "\n",
    "# init bias\n",
    "bias_i = getattr(lstm, 'bias_hh_l{0}'.format(0))\n",
    "torch.nn.init.constant_(bias_i, 0.1)\n",
    "bias_h = getattr(lstm, 'bias_ih_l{0}'.format(0))\n",
    "torch.nn.init.constant_(bias_h, 0.1)\n",
    "\n",
    "bias_i_r = getattr(lstm, 'bias_hh_l{0}_reverse'.format(0))\n",
    "bias_i_r.data.copy_(bias_i)\n",
    "bias_h_r = getattr(lstm, 'bias_ih_l{0}_reverse'.format(0))\n",
    "bias_h_r.data.copy_(bias_h)\n",
    "\n",
    "print('Initialization is done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样，若某个序列的正、反向输入相同，则正、反方向最后一个时刻的输出应该一致；若反向LSTM计算了`_PAD`值，则输出结果会不一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建模拟数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 64])\n",
      "inputs_packed.data.size: torch.Size([10, 64])\n",
      "batch_sizes: tensor([ 3,  3,  2,  1,  1])\n"
     ]
    }
   ],
   "source": [
    "# 设置模拟输入数据\n",
    "sentences = [['nice', 'day'], ['I', 'like', 'to', 'eat', 'apple'], ['can', 'a', 'can']]\n",
    "test_sent_idx = 2  # 即['can', 'a', 'can']在sentences中的下标\n",
    "\n",
    "# 构建alphabet\n",
    "alphabet = {}\n",
    "index = 1\n",
    "for sentence in sentences:\n",
    "    for word in sentence:\n",
    "        if word not in alphabet:\n",
    "            alphabet[word] = index\n",
    "            index += 1\n",
    "voc_size = len(alphabet) + 1\n",
    "\n",
    "lengths = [len(s) for s in sentences]\n",
    "max_len = max(lengths)\n",
    "batch_size = len(sentences)\n",
    "\n",
    "inputs = np.zeros((batch_size, max_len), dtype='int32')\n",
    "for i, sentence in enumerate(sentences):\n",
    "    ids = list(map(lambda w: alphabet[w], sentence))\n",
    "    inputs[i, :lengths[i]] = ids\n",
    "inputs = torch.LongTensor(inputs)\n",
    "lengths = torch.LongTensor(lengths)\n",
    "\n",
    "# 按句子实际长度降序排序\n",
    "lengths, indices = torch.sort(lengths, descending=True)\n",
    "inputs = inputs[indices]\n",
    "\n",
    "# 设置embedding层，其中padding_idx表示padding值的编号\n",
    "embedding = nn.Embedding(voc_size, input_size, padding_idx=0)\n",
    "inputs = embedding(inputs)\n",
    "print(inputs.size())  # [3, 5, 64]\n",
    "\n",
    "inputs_packed = pack_padded_sequence(inputs, lengths, True)\n",
    "print('inputs_packed.data.size: {0}'.format(inputs_packed.data.size()))\n",
    "print('batch_sizes: {0}'.format(inputs_packed.batch_sizes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PackedSequence包含两个值，分别是`data`和`batch_sizes`。其中`data`根据`lengths`参数(即序列的实际长度)，记录了`inputs`中的tensor；`batch_sizes`长度等于实际长度的最大值，第`i`个值记录了第`i`时刻输入的batch size大小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算LSTM输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 100])\n"
     ]
    }
   ],
   "source": [
    "lstm_output, lstm_hidden = lstm(inputs_packed)\n",
    "lstm_hidden, lstm_cell_state = lstm_hidden[0], lstm_hidden[1]\n",
    "lstm_hidden = lstm_hidden.transpose(0, 1)\n",
    "print(lstm_hidden.size())  # torch.Size([batch_size, 2, hidden_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`lstm_hidden`和`lstm_cell_state`分别记录了正、反向最后一个时刻的`hidden state`和`cell state`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0466,  0.1979,  0.1674,  0.1115, -0.1394, -0.3406,  0.0461,\n",
      "          0.1912,  0.0237, -0.0359, -0.0968, -0.2336, -0.0023, -0.1093,\n",
      "          0.1821,  0.0722, -0.2315,  0.2709,  0.3707, -0.0824,  0.1123,\n",
      "         -0.1798, -0.0687, -0.0292,  0.0763,  0.1033, -0.4348,  0.0839,\n",
      "          0.2364,  0.2123,  0.2637, -0.1207,  0.1106,  0.2471,  0.0100,\n",
      "          0.1297, -0.2408,  0.0088,  0.0758,  0.0115, -0.0822,  0.0283,\n",
      "          0.1517,  0.1999,  0.0125, -0.0514, -0.0111,  0.0573,  0.1063,\n",
      "          0.2532,  0.4012,  0.2466,  0.3506,  0.3177,  0.1393, -0.2369,\n",
      "          0.0339,  0.1768, -0.0028,  0.2993,  0.3876, -0.1459,  0.1071,\n",
      "         -0.0458,  0.1388, -0.1838, -0.0033,  0.2611, -0.0364,  0.0620,\n",
      "          0.0998,  0.4584,  0.1556, -0.0932,  0.3981,  0.1540,  0.1624,\n",
      "         -0.0202,  0.0649,  0.2544,  0.0112, -0.2623, -0.0341,  0.0671,\n",
      "          0.0608, -0.1244, -0.1278, -0.0176,  0.2964, -0.1301,  0.2018,\n",
      "          0.3965,  0.2164,  0.2954,  0.1335,  0.1810, -0.0348,  0.0848,\n",
      "          0.1316, -0.0578],\n",
      "        [-0.0466,  0.1979,  0.1674,  0.1115, -0.1394, -0.3406,  0.0461,\n",
      "          0.1912,  0.0237, -0.0359, -0.0968, -0.2336, -0.0023, -0.1093,\n",
      "          0.1821,  0.0722, -0.2315,  0.2709,  0.3707, -0.0824,  0.1123,\n",
      "         -0.1798, -0.0687, -0.0292,  0.0763,  0.1033, -0.4348,  0.0839,\n",
      "          0.2364,  0.2123,  0.2637, -0.1207,  0.1106,  0.2471,  0.0100,\n",
      "          0.1297, -0.2408,  0.0088,  0.0758,  0.0115, -0.0822,  0.0283,\n",
      "          0.1517,  0.1999,  0.0125, -0.0514, -0.0111,  0.0573,  0.1063,\n",
      "          0.2532,  0.4012,  0.2466,  0.3506,  0.3177,  0.1393, -0.2369,\n",
      "          0.0339,  0.1768, -0.0028,  0.2993,  0.3876, -0.1459,  0.1071,\n",
      "         -0.0458,  0.1388, -0.1838, -0.0033,  0.2611, -0.0364,  0.0620,\n",
      "          0.0998,  0.4584,  0.1556, -0.0932,  0.3981,  0.1540,  0.1624,\n",
      "         -0.0202,  0.0649,  0.2544,  0.0112, -0.2623, -0.0341,  0.0671,\n",
      "          0.0608, -0.1244, -0.1278, -0.0176,  0.2964, -0.1301,  0.2018,\n",
      "          0.3965,  0.2164,  0.2954,  0.1335,  0.1810, -0.0348,  0.0848,\n",
      "          0.1316, -0.0578]])\n"
     ]
    }
   ],
   "source": [
    "# lstm_hidden还原为原来的顺序\n",
    "_, indices_recover = torch.sort(indices)\n",
    "lstm_hidden_recover = lstm_hidden[indices_recover]\n",
    "\n",
    "# 句子['can', 'a', 'can']正反向lstm最后一个时刻的输出\n",
    "print(lstm_hidden_recover[test_sent_idx])  # [2, 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出正、反向的输出一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`lstm_output`记录的是BLSTM每个时刻的输出，根据句子实际长度，也可以取出在最后一个时刻的输出："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 200])\n",
      "tensor([ 0.4026,  0.0825, -0.0540,  0.1774, -0.2190,  0.3484, -0.0861,\n",
      "        -0.1873,  0.1525, -0.2399, -0.1848,  0.3643,  0.0609, -0.1586,\n",
      "         0.2929,  0.2200,  0.0576,  0.3060,  0.1052,  0.1356,  0.0735,\n",
      "         0.0590,  0.0248,  0.1299,  0.3680, -0.0334,  0.2734,  0.1144,\n",
      "         0.2946, -0.1674, -0.0226,  0.1603, -0.0575,  0.2273,  0.0773,\n",
      "         0.3728,  0.0548,  0.0149,  0.0774, -0.1232,  0.1091,  0.3588,\n",
      "         0.1203,  0.2008,  0.0501,  0.1625,  0.2344,  0.4315,  0.1669,\n",
      "         0.1597,  0.1626,  0.1740, -0.0047,  0.0115, -0.0905,  0.2055,\n",
      "        -0.1133, -0.1211, -0.0808,  0.1940,  0.1274, -0.0705,  0.3216,\n",
      "         0.1586,  0.1263,  0.0813,  0.1671,  0.0787,  0.1572,  0.0974,\n",
      "         0.2064,  0.1374,  0.0428,  0.0720,  0.1243,  0.3860,  0.1179,\n",
      "         0.0071, -0.0972,  0.0142,  0.0914,  0.2337,  0.2610, -0.1452,\n",
      "         0.1883,  0.1320, -0.0422,  0.2024, -0.0048, -0.0715,  0.2454,\n",
      "        -0.1260,  0.1222,  0.1351,  0.0488,  0.2583, -0.0402, -0.1891,\n",
      "        -0.0738,  0.2203])\n",
      "tensor([ 0.4026,  0.0825, -0.0540,  0.1774, -0.2190,  0.3484, -0.0861,\n",
      "        -0.1873,  0.1525, -0.2399, -0.1848,  0.3643,  0.0609, -0.1586,\n",
      "         0.2929,  0.2200,  0.0576,  0.3060,  0.1052,  0.1356,  0.0735,\n",
      "         0.0590,  0.0248,  0.1299,  0.3680, -0.0334,  0.2734,  0.1144,\n",
      "         0.2946, -0.1674, -0.0226,  0.1603, -0.0575,  0.2273,  0.0773,\n",
      "         0.3728,  0.0548,  0.0149,  0.0774, -0.1232,  0.1091,  0.3588,\n",
      "         0.1203,  0.2008,  0.0501,  0.1625,  0.2344,  0.4315,  0.1669,\n",
      "         0.1597,  0.1626,  0.1740, -0.0047,  0.0115, -0.0905,  0.2055,\n",
      "        -0.1133, -0.1211, -0.0808,  0.1940,  0.1274, -0.0705,  0.3216,\n",
      "         0.1586,  0.1263,  0.0813,  0.1671,  0.0787,  0.1572,  0.0974,\n",
      "         0.2064,  0.1374,  0.0428,  0.0720,  0.1243,  0.3860,  0.1179,\n",
      "         0.0071, -0.0972,  0.0142,  0.0914,  0.2337,  0.2610, -0.1452,\n",
      "         0.1883,  0.1320, -0.0422,  0.2024, -0.0048, -0.0715,  0.2454,\n",
      "        -0.1260,  0.1222,  0.1351,  0.0488,  0.2583, -0.0402, -0.1891,\n",
      "        -0.0738,  0.2203])\n"
     ]
    }
   ],
   "source": [
    "# 还原为原形状\n",
    "lstm_output_pad, lengths = pad_packed_sequence(lstm_output, batch_first=True)\n",
    "lstm_output_pad_recover = lstm_output_pad[indices_recover]\n",
    "lengths_recover = lengths[indices_recover]\n",
    "print(lstm_output_pad.size())  # size=[3, 5, 200]\n",
    "\n",
    "# 句子['can', 'a', 'can']正反向lstm最后一个时刻的输出\n",
    "hidden_last = lstm_output_pad_recover[test_sent_idx][lengths_recover[test_sent_idx]-1][:hidden_size]  # 正向\n",
    "hidden_last_r = lstm_output_pad_recover[test_sent_idx][0][hidden_size:]  # 反向\n",
    "print(hidden_last)\n",
    "print(hidden_last_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "取出的值与`lstm_hidden`的值相等。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
