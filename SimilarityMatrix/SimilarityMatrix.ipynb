{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用深度学习建模句子对相似度任务时，通常有两种方式，分别是`Representation-based model`和`Interaction-based model`。前者分别将两个句子独立编码为语义向量，然后再判断句子之间的相似度；后者首先计算两个句子的相似度矩阵，再利用更复杂的模型捕获相似度特征，从而判断句子之间的相似度。\n",
    "\n",
    "这里，我们仅考虑`Interaction-based model`最简单的形式，即两个句子分别表示成词向量，再计算句子之间的相似度矩阵。\n",
    "\n",
    "以下是PyTorch的实现方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 模拟数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下数据取自NIPS 2011论文：[Dynamic pooling and unfolding recursive autoencoders for paraphrase detection](http://papers.nips.cc/paper/4204-dynamic-pooling-and-unfolding-recursive-autoencoders-for-paraphrase-detection.pdf)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 23\n"
     ]
    }
   ],
   "source": [
    "sentence_pairs_ = [\n",
    "    (\n",
    "        'LLEYTON Hewitt yesterday traded his tennis racquet for his first sporting passion - ' + \\\n",
    "          'Australian football - as the world champion relaxed before his Wimbledon title defence',\n",
    "        'LLEYTON Hewitt yesterday traded his tennis racquet for his first sporting passionAustralian ' + \\\n",
    "          'rules football-as the world champion relaxed ahead of his Wimbledon defence'\n",
    "    ),\n",
    "    (\n",
    "        'The lies and deceptions from Saddam have been well documented over 12 years',\n",
    "        'It has been well documented over 12 years of lies and deception from Saddam'\n",
    "    ),\n",
    "    ('The initial report was made to Modesto Police December 28', 'It stems from a Modesto police report')\n",
    "]\n",
    "sentence_pairs = [[] for i in range(len(sentence_pairs_))]\n",
    "max_len_1, max_len_2 = -1, -1\n",
    "for i, pair in enumerate(sentence_pairs_):\n",
    "    words = pair[0].split(' ')\n",
    "    sentence_pairs[i].append(words)\n",
    "    if len(words) > max_len_1:\n",
    "        max_len_1 = len(words)\n",
    "    words = pair[1].split(' ')\n",
    "    sentence_pairs[i].append(words)\n",
    "    if len(words) > max_len_2:\n",
    "        max_len_2 = len(words)\n",
    "print(max_len_1, max_len_2)  # 26 23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 构建alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stems': 54, 'first': 9, 'as': 15, 'Modesto': 50, 'of': 28, '-': 12, 'relaxed': 19, 'to': 49, 'been': 36, 'December': 52, 'defence': 23, 'traded': 4, 'LLEYTON': 1, 'passionAustralian': 24, 'a': 55, 'football-as': 26, 'report': 46, 'Saddam': 34, 'was': 47, 'passion': 11, 'for': 8, 'has': 43, 'well': 37, 'Wimbledon': 21, 'have': 35, '28': 53, 'ahead': 27, 'the': 16, 'lies': 30, 'made': 48, 'Hewitt': 2, 'racquet': 7, 'deceptions': 32, 'title': 22, 'before': 20, 'initial': 45, 'world': 17, 'from': 33, 'The': 29, 'police': 56, 'rules': 25, 'over': 39, '12': 40, 'deception': 44, 'documented': 38, 'and': 31, 'sporting': 10, 'champion': 18, 'tennis': 6, 'Australian': 13, 'It': 42, 'his': 5, 'football': 14, 'years': 41, 'Police': 51, 'yesterday': 3}\n"
     ]
    }
   ],
   "source": [
    "# 构建alphabet\n",
    "alphabet = dict()\n",
    "index = 1\n",
    "for pair in sentence_pairs:\n",
    "    for word in pair[0] + pair[1]:\n",
    "        if word not in alphabet:\n",
    "            alphabet[word] = index\n",
    "            index += 1\n",
    "print(alphabet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 词序列转为id序列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义将词序列转为id序列的函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2ids(words):\n",
    "    ids = []\n",
    "    for word in words:\n",
    "        if word in alphabet:\n",
    "            ids.append(alphabet[word])\n",
    "        else:\n",
    "            ids.append(0)\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将`sentence_pairs`转为词序列："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "sentences_1 = np.zeros((len(sentence_pairs), max_len_1), dtype='int32')\n",
    "sentences_2 = np.zeros((len(sentence_pairs), max_len_2), dtype='int32')\n",
    "\n",
    "for i, pair in enumerate(sentence_pairs):\n",
    "    ids = word2ids(pair[0])\n",
    "    sentences_1[i][:len(ids)] = ids\n",
    "    ids = word2ids(pair[1])\n",
    "    sentences_2[i][:len(ids)] = ids\n",
    "\n",
    "sentences_1 = torch.LongTensor(sentences_1)\n",
    "sentences_2 = torch.LongTensor(sentences_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先加载预训练词向量，使用的是[glove（400k,50维）](https://nlp.stanford.edu/projects/glove/)，工具为gensim。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "400000\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('./data/glove.6B.50d.bin', binary=True)\n",
    "print(word2vec_model.vector_size)  # 50\n",
    "print(len(word2vec_model.vocab))  # 400000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 从预训练词向量构建embedding层权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exact match: 44, case match: 10, miss match: 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "embedding_weights = np.zeros((len(alphabet)+1, word2vec_model.vector_size), dtype='float32')  # shape=[57, 50]\n",
    "\n",
    "exact_match, case_match, miss_match = 0, 0, 0 \n",
    "for word in alphabet:\n",
    "    if word in word2vec_model:\n",
    "        embedding_weights[alphabet[word]] = word2vec_model[word]\n",
    "        exact_match += 1\n",
    "    elif word.lower() in word2vec_model:\n",
    "        embedding_weights[alphabet[word]] = word2vec_model[word.lower()]\n",
    "        case_match += 1\n",
    "    else:\n",
    "        miss_match += 1\n",
    "print('exact match: {0}, case match: {1}, miss match: {2}'.format(exact_match, case_match, miss_match))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 构建Embedding层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(57, 50, padding_idx=0)\n",
      "torch.Size([3, 26, 50])\n",
      "torch.Size([3, 23, 50])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "embedding = nn.Embedding(len(alphabet)+1, word2vec_model.vector_size, padding_idx=0)\n",
    "embedding.weight.data.copy_(torch.FloatTensor(embedding_weights))\n",
    "\n",
    "print(embedding)\n",
    "print(embedding(sentences_1).size())  # size=[3, 26, 50]\n",
    "print(embedding(sentences_2).size())  # size=[3, 23, 50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 计算Similarity Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义计算相似度矩阵的函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity_matrix(input_1, input_2):\n",
    "        \"\"\"获取input_1与input_2之间的余弦相似度矩阵\n",
    "        Args:\n",
    "            input_1: 3D tensor, size=[bs, max_len_1, input_dim]\n",
    "            input_2: 3D tensor, size=[bs, max_len_2, input_dim]\n",
    "\n",
    "        Returns:\n",
    "            similarity_matrix: 3D tensor, size=[bs, max_len_1, max_len_2]\n",
    "        \"\"\"\n",
    "        # 计算分子, size=[bs, max_len_1, max_len_2]\n",
    "        similarity_part_1 = torch.bmm(\n",
    "            input_1, torch.transpose(input_2, 2, 1).contiguous())\n",
    "\n",
    "        # 计算分母\n",
    "        input_sqrt_sum_pow_1 = torch.sqrt(\n",
    "            torch.sum(torch.pow(input_1, 2), -1)).unsqueeze(-1)  # [bs, max_len_1, 1]\n",
    "        input_sqrt_sum_pow_2 = torch.sqrt(\n",
    "            torch.sum(torch.pow(input_2, 2), -1)).unsqueeze(-2)  # [bs, 1, max_len_2]\n",
    "        similarity_part_2 = torch.bmm(\n",
    "            input_sqrt_sum_pow_1, input_sqrt_sum_pow_2)  # [bs, max_len_1, max_len_2]\n",
    "\n",
    "        # 计算cosine similarity，需要考虑除0的情况，防止出现`nan`\n",
    "        return similarity_part_1 / (similarity_part_2 + 1.e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将id序列转换为embedding形式，并计算相似度矩阵："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 26, 23])\n"
     ]
    }
   ],
   "source": [
    "input_1 = embedding(sentences_1)\n",
    "input_2 = embedding(sentences_2)\n",
    "\n",
    "similarity_matrix = compute_similarity_matrix(input_1, input_2)\n",
    "print(similarity_matrix.size())  # size=[3, 26, 23]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "similarity_matrix[i][j][k]即表示第i对句子中，句子1中的地j个词与句子2中的第k个词之间的余弦相似度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 可视化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，可以利用matplotlib库将相似度矩阵可视化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7f04c9007d68>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm as cm\n",
    "\n",
    "sent_idx = 0\n",
    "fig, ax = plt.subplots(figsize=(max_len_1,max_len_2))\n",
    "# cmap = cm.get_cmap('Greens')\n",
    "cax = ax.matshow(similarity_matrix[sent_idx].data, interpolation='nearest', cmap=None)\n",
    "plt.xticks(range(max_len_2), sentence_pairs[sent_idx][1], rotation=90)\n",
    "plt.yticks(range(max_len_1), sentence_pairs[sent_idx][0])\n",
    "fig.colorbar(cax, ticks=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, .75,.8,.85,.90,.95,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图中，颜色越红表示相似度越高，颜色越蓝表示相似度越低，可以看出对角线附近出现很多深红色的方块。另外，在`passionAustralian`和`football-as`这两列为全蓝，这是因为这两个词是未登录词，我们在上文将未登录词的词向量初始化为全0，所以未登录词与任何词的余弦相似度都是很小的值。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
